{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44d24bd-bed7-4b6f-a5b0-120a430e843c",
   "metadata": {},
   "source": [
    "# Part 1: Data Ingestion\n",
    "## Programmatic Download (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087cadd0-088d-4663-b0fd-f2d70525e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310fb228-6fca-4a2e-ba83-945219c05a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767a0538-a74d-4b58-b4bb-920b9f15dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"data/raw\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trip_data_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "trip_data_file = data_dir / \"yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "zone_data_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "zone_data_file = data_dir / \"taxi_zone_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b70432-c054-4186-b3de-9cebe8f1a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, file_path):\n",
    "    if not file_path.exists():\n",
    "        print(f\"Downloading from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        file_size_mb = os.path.getsize(file_path) / 1e6\n",
    "        print(f\"Downloaded: {file_path.name} ({file_size_mb:.1f} MB)\")\n",
    "        return True\n",
    "    else:\n",
    "        file_size_mb = os.path.getsize(file_path) / 1e6\n",
    "        print(f\"File already exists: {file_path.name} ({file_size_mb:.1f} MB)\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263bb952-4260-42ac-b440-cf1de935e93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: yellow_tripdata_2024-01.parquet (50.0 MB)\n",
      "File already exists: taxi_zone_lookup.csv (0.0 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file(trip_data_url, trip_data_file)\n",
    "\n",
    "download_file(zone_data_url, zone_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a2f0b-ae8b-4318-acc8-d24e9b63235b",
   "metadata": {},
   "source": [
    "## Data Validation (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0670bbb5-df3b-4d4f-bbaf-accb8077c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_TRIP_COLUMNS = [\n",
    "    'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'fare_amount',\n",
    "    'tip_amount',\n",
    "    'total_amount',\n",
    "    'payment_type'\n",
    "]\n",
    "\n",
    "EXPECTED_ZONE_COLUMNS = [\n",
    "    'LocationID',\n",
    "    'Borough',\n",
    "    'Zone',\n",
    "    'service_zone'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ea8e6d-2c85-4386-a4d5-14be2c05cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully\n",
      "Shape: 2964624 rows × 19 columns\n"
     ]
    }
   ],
   "source": [
    "def load_data (file_path):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"✗ Error file does not exist: {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        df = pl.read_parquet(file_path)\n",
    "        print(f\"File loaded successfully\")\n",
    "        print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return False\n",
    "\n",
    "load_data(trip_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d207fd8-d720-48b9-9b21-a7399dc27c9e",
   "metadata": {},
   "source": [
    "### a) Verify all expected columns exist in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b4b8220-f1b3-42d1-8a2c-662af2769f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2964624\n",
      "\n",
      "Validating Trip Data...\n",
      "File: data\\raw\\yellow_tripdata_2024-01.parquet\n",
      "File loaded successfully\n",
      "Shape: 2964624 rows × 19 columns\n",
      "The Expected Columns are their\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet(trip_data_file)\n",
    "print(df.height)\n",
    "\n",
    "def verify_trip_columns(file_path, expected_columns):\n",
    "    print(f\"\\nValidating Trip Data...\")\n",
    "    print(f\"File: {file_path}\")\n",
    "\n",
    "    load_data(trip_data_file)\n",
    "\n",
    "    df = pl.read_parquet(file_path)\n",
    "    \n",
    "    #Part A\n",
    "    \n",
    "    actual_columns = df.columns\n",
    "    \n",
    "    missing_columns = []\n",
    "    for col in expected_columns:\n",
    "        if col not in actual_columns:\n",
    "            missing_columns.append(col)\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Their are {len(missing_columns)} missing Colums\")\n",
    "        print(f\"Actual columns: {actual_columns}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"The Expected Columns are their\")\n",
    "    \n",
    "        return True\n",
    "\n",
    "trip_columns_valid = verify_trip_columns(trip_data_file, EXPECTED_TRIP_COLUMNS)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cdefe96-da93-4263-8071-9c21642f9009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Zone Lookup Table\n",
      "File: data\\raw\\taxi_zone_lookup.csv\n",
      "File loaded successfully\n",
      "  Shape: 265 rows × 4 columns\n",
      "The Expected Columns are their\n"
     ]
    }
   ],
   "source": [
    "def verify_zone_columns(file_path, expected_columns):\n",
    "    print(f\"\\nValidating Zone Lookup Table\")\n",
    "    print(f\"File: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Check file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"✗ ERROR: File not found: {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Load CSV file with Pandas\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"File loaded successfully\")\n",
    "        print(f\"  Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "        \n",
    "        actual_columns = df.columns.tolist()\n",
    "        \n",
    "        # Find missing columns\n",
    "        missing_columns = [col for col in expected_columns if col not in actual_columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Their are {len(missing_columns)} missing Colums\")\n",
    "            print(f\"  Available columns: {actual_columns}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"The Expected Columns are their\")\n",
    "            \n",
    "            # Show extra columns (if any)\n",
    "            extra_columns = [col for col in actual_columns if col not in expected_columns]\n",
    "            if extra_columns:\n",
    "                print(f\"  Note: Found {len(extra_columns)} additional columns:\")\n",
    "                for col in extra_columns:\n",
    "                    print(f\"    - {col}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error file did'nt load {e}\")\n",
    "        return False\n",
    "\n",
    "zone_columns_valid = verify_zone_columns(zone_data_file, EXPECTED_ZONE_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70db9365-9e9d-4921-9102-c63ab862235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deoad\\AppData\\Local\\Temp\\ipykernel_22612\\3752748169.py:6: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  dtype = df.schema.get(col)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_datetime_cols(df, expected_cols):\n",
    "\n",
    "    df = pl.scan_parquet(df)\n",
    "    \n",
    "    for col in expected_cols:\n",
    "        dtype = df.schema.get(col)\n",
    "\n",
    "        if dtype is None:\n",
    "            print(f\"{col} missing\")\n",
    "            return False\n",
    "\n",
    "        if dtype.base_type() != pl.Datetime:\n",
    "            print(f\"{col} wrong type: {dtype}\")\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "expected_datetime_cols = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "validate_datetime_cols(trip_data_file, expected_datetime_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d3e08a-9e1d-420e-bf6e-38acd237f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Trip Data...\n",
      "File: data\\raw\\yellow_tripdata_2024-01.parquet\n",
      "File loaded successfully\n",
      "Shape: 2964624 rows × 19 columns\n",
      "The Expected Columns are their\n",
      "\n",
      "Validating Zone Lookup Table\n",
      "File: data\\raw\\taxi_zone_lookup.csv\n",
      "File loaded successfully\n",
      "  Shape: 265 rows × 4 columns\n",
      "The Expected Columns are their\n",
      "\n",
      "\n",
      "Summary:\n",
      "\n",
      "\n",
      "All required columns are present in both datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deoad\\AppData\\Local\\Temp\\ipykernel_22612\\3752748169.py:6: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  dtype = df.schema.get(col)\n"
     ]
    }
   ],
   "source": [
    "# Validate trip data columns\n",
    "trip_columns_valid = verify_trip_columns(trip_data_file, EXPECTED_TRIP_COLUMNS)\n",
    "\n",
    "# Validate zone data columns  \n",
    "zone_columns_valid = verify_zone_columns(zone_data_file, EXPECTED_ZONE_COLUMNS)\n",
    "\n",
    "# Validate datetime types\n",
    "expected_datetime_cols = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "datatime_colums_valid = validate_datetime_cols(trip_data_file, expected_datetime_cols)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\\nSummary:\\n\\n\")\n",
    "if trip_columns_valid and zone_columns_valid and datatime_colums_valid:\n",
    "    print(\"All required columns are present in both datasets\")\n",
    "else:\n",
    "    print(\"Error unvalidated required columns\")\n",
    "    \n",
    "    if not trip_columns_valid:\n",
    "        print(\"  - Trip data: Column validation failed\")\n",
    "    \n",
    "    if not zone_columns_valid:\n",
    "        print(\"  - Zone lookup: Column validation failed\")\n",
    "\n",
    "    if not datatime_colums_valid:\n",
    "        print(\"  - Datetime : Column validation failed\")\n",
    "    \n",
    "    print(\"\\nExiting due to validation failures.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b4309-fb9a-4b3b-941a-524d93695e0e",
   "metadata": {},
   "source": [
    "# Part 2: Data Transformation & Analysis (30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba7d5c-d3c3-4c26-b5d3-bf181ee0bd8d",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eb7e2-3a92-4869-9b54-3ba1043b1f12",
   "metadata": {},
   "source": [
    "### e) Removing rows with null values in critical columns (pickup/dropoff times, locations, fare) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385bebbc-c363-4897-83ad-24d0b8a042e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the data before dropping null vaules: 2964624\n",
      "This is the data after dropping null vaules: 2964624\n",
      "Amount dropped: 0\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_parquet(trip_data_file)\n",
    "cur = df.height\n",
    "\n",
    "print(f\"This is the data before dropping null vaules: {cur}\")\n",
    "\n",
    "clean = df.drop_nulls(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\", \"fare_amount\"])\n",
    "print(f\"This is the data after dropping null vaules: {clean.height}\")\n",
    "print(f\"Amount dropped: {cur - clean.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b690c4-32c1-4e3c-84d3-ad948cf28d74",
   "metadata": {},
   "source": [
    "### f) Filtering out invalid trips: trips with zero or negative distance, negative fares, or fares exceeding $500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5e6bdf7-0814-4eea-b823-3935fae82773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current rows before removing invaild distances are 2964624\n",
      "The cleaned distances are 60371\n",
      "The current rows before removing invaild fares are 2904253\n",
      "The cleaned fares are 34573\n",
      "The current rows before removing invaild dates are 2869680\n",
      "The cleaned dates are 112\n"
     ]
    }
   ],
   "source": [
    "# Filtering out invalid trips: trips with zero or negative distance\n",
    "cur = clean.height\n",
    "print(f\"The current rows before removing invaild distances are {cur}\")\n",
    "clean = clean.filter(pl.col(\"trip_distance\").is_not_null() & (pl.col(\"trip_distance\") > 0))\n",
    "invalid_distances = cur - clean.height\n",
    "print(f\"The cleaned distances are {invalid_distances}\")\n",
    "\n",
    "# Filtering out invalid trips: trips with negative fares, or fares exceeding $500 \n",
    "cur = clean.height\n",
    "print(f\"The current rows before removing invaild fares are {cur}\")\n",
    "clean = clean.filter((pl.col('fare_amount') > 0) & (pl.col('fare_amount') < 500))\n",
    "invalid_fares = cur - clean.height\n",
    "print(f\"The cleaned fares are {invalid_fares}\")\n",
    "\n",
    "# Removing trips where dropoff time is before pickup time \n",
    "cur = clean.height\n",
    "print(f\"The current rows before removing invaild dates are {cur}\")\n",
    "clean = clean.filter(pl.col('tpep_dropoff_datetime') > pl.col('tpep_pickup_datetime'))\n",
    "invalid_dates = cur - clean.height\n",
    "print(f\"The cleaned dates are {invalid_dates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf283ee-ce9b-4f51-a427-db4fa636d67a",
   "metadata": {},
   "source": [
    "## Feature Engineering (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656fe29d-bd29-45bb-882c-f1c15b2261ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_964_624, 23)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>VendorID</th><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>RatecodeID</th><th>store_and_fwd_flag</th><th>PULocationID</th><th>DOLocationID</th><th>payment_type</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th><th>congestion_surcharge</th><th>Airport_fee</th><th>trip_duration_minutes</th><th>trip_speed_mph</th><th>pickup_hour</th><th>pickup_day_of_week</th></tr><tr><td>i32</td><td>datetime[ns]</td><td>datetime[ns]</td><td>i64</td><td>f64</td><td>i64</td><td>str</td><td>i32</td><td>i32</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td><td>str</td></tr></thead><tbody><tr><td>2</td><td>2024-01-01 00:57:55</td><td>2024-01-01 01:17:43</td><td>1</td><td>1.72</td><td>1</td><td>&quot;N&quot;</td><td>186</td><td>79</td><td>2</td><td>17.7</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>22.7</td><td>2.5</td><td>0.0</td><td>19.8</td><td>5.212121</td><td>0</td><td>&quot;Monday&quot;</td></tr><tr><td>1</td><td>2024-01-01 00:03:00</td><td>2024-01-01 00:09:36</td><td>1</td><td>1.8</td><td>1</td><td>&quot;N&quot;</td><td>140</td><td>236</td><td>1</td><td>10.0</td><td>3.5</td><td>0.5</td><td>3.75</td><td>0.0</td><td>1.0</td><td>18.75</td><td>2.5</td><td>0.0</td><td>6.6</td><td>16.363636</td><td>0</td><td>&quot;Monday&quot;</td></tr><tr><td>1</td><td>2024-01-01 00:17:06</td><td>2024-01-01 00:35:01</td><td>1</td><td>4.7</td><td>1</td><td>&quot;N&quot;</td><td>236</td><td>79</td><td>1</td><td>23.3</td><td>3.5</td><td>0.5</td><td>3.0</td><td>0.0</td><td>1.0</td><td>31.3</td><td>2.5</td><td>0.0</td><td>17.916667</td><td>15.739535</td><td>0</td><td>&quot;Monday&quot;</td></tr><tr><td>1</td><td>2024-01-01 00:36:38</td><td>2024-01-01 00:44:56</td><td>1</td><td>1.4</td><td>1</td><td>&quot;N&quot;</td><td>79</td><td>211</td><td>1</td><td>10.0</td><td>3.5</td><td>0.5</td><td>2.0</td><td>0.0</td><td>1.0</td><td>17.0</td><td>2.5</td><td>0.0</td><td>8.3</td><td>10.120482</td><td>0</td><td>&quot;Monday&quot;</td></tr><tr><td>1</td><td>2024-01-01 00:46:51</td><td>2024-01-01 00:52:57</td><td>1</td><td>0.8</td><td>1</td><td>&quot;N&quot;</td><td>211</td><td>148</td><td>1</td><td>7.9</td><td>3.5</td><td>0.5</td><td>3.2</td><td>0.0</td><td>1.0</td><td>16.1</td><td>2.5</td><td>0.0</td><td>6.1</td><td>7.868852</td><td>0</td><td>&quot;Monday&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2</td><td>2024-01-31 23:45:59</td><td>2024-01-31 23:54:36</td><td>null</td><td>3.18</td><td>null</td><td>null</td><td>107</td><td>263</td><td>0</td><td>15.77</td><td>0.0</td><td>0.5</td><td>2.0</td><td>0.0</td><td>1.0</td><td>21.77</td><td>null</td><td>null</td><td>8.616667</td><td>22.143133</td><td>23</td><td>&quot;Wednesday&quot;</td></tr><tr><td>1</td><td>2024-01-31 23:13:07</td><td>2024-01-31 23:27:52</td><td>null</td><td>4.0</td><td>null</td><td>null</td><td>114</td><td>236</td><td>0</td><td>18.4</td><td>1.0</td><td>0.5</td><td>2.34</td><td>0.0</td><td>1.0</td><td>25.74</td><td>null</td><td>null</td><td>14.75</td><td>16.271186</td><td>23</td><td>&quot;Wednesday&quot;</td></tr><tr><td>2</td><td>2024-01-31 23:19:00</td><td>2024-01-31 23:38:00</td><td>null</td><td>3.33</td><td>null</td><td>null</td><td>211</td><td>25</td><td>0</td><td>19.97</td><td>0.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>1.0</td><td>23.97</td><td>null</td><td>null</td><td>19.0</td><td>10.515789</td><td>23</td><td>&quot;Wednesday&quot;</td></tr><tr><td>2</td><td>2024-01-31 23:07:23</td><td>2024-01-31 23:25:14</td><td>null</td><td>3.06</td><td>null</td><td>null</td><td>107</td><td>13</td><td>0</td><td>23.88</td><td>0.0</td><td>0.5</td><td>5.58</td><td>0.0</td><td>1.0</td><td>33.46</td><td>null</td><td>null</td><td>17.85</td><td>10.285714</td><td>23</td><td>&quot;Wednesday&quot;</td></tr><tr><td>1</td><td>2024-01-31 23:58:25</td><td>2024-02-01 00:13:30</td><td>null</td><td>8.1</td><td>null</td><td>null</td><td>138</td><td>75</td><td>0</td><td>32.4</td><td>7.75</td><td>0.5</td><td>7.29</td><td>6.94</td><td>1.0</td><td>55.88</td><td>null</td><td>null</td><td>15.083333</td><td>32.220994</td><td>23</td><td>&quot;Wednesday&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_964_624, 23)\n",
       "┌──────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ VendorID ┆ tpep_pick ┆ tpep_drop ┆ passenger ┆ … ┆ trip_dura ┆ trip_spee ┆ pickup_ho ┆ pickup_da │\n",
       "│ ---      ┆ up_dateti ┆ off_datet ┆ _count    ┆   ┆ tion_minu ┆ d_mph     ┆ ur        ┆ y_of_week │\n",
       "│ i32      ┆ me        ┆ ime       ┆ ---       ┆   ┆ tes       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│          ┆ ---       ┆ ---       ┆ i64       ┆   ┆ ---       ┆ f64       ┆ i8        ┆ str       │\n",
       "│          ┆ datetime[ ┆ datetime[ ┆           ┆   ┆ f64       ┆           ┆           ┆           │\n",
       "│          ┆ ns]       ┆ ns]       ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "╞══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2        ┆ 2024-01-0 ┆ 2024-01-0 ┆ 1         ┆ … ┆ 19.8      ┆ 5.212121  ┆ 0         ┆ Monday    │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 00:57:55  ┆ 01:17:43  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 1        ┆ 2024-01-0 ┆ 2024-01-0 ┆ 1         ┆ … ┆ 6.6       ┆ 16.363636 ┆ 0         ┆ Monday    │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 00:03:00  ┆ 00:09:36  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 1        ┆ 2024-01-0 ┆ 2024-01-0 ┆ 1         ┆ … ┆ 17.916667 ┆ 15.739535 ┆ 0         ┆ Monday    │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 00:17:06  ┆ 00:35:01  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 1        ┆ 2024-01-0 ┆ 2024-01-0 ┆ 1         ┆ … ┆ 8.3       ┆ 10.120482 ┆ 0         ┆ Monday    │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 00:36:38  ┆ 00:44:56  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 1        ┆ 2024-01-0 ┆ 2024-01-0 ┆ 1         ┆ … ┆ 6.1       ┆ 7.868852  ┆ 0         ┆ Monday    │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 00:46:51  ┆ 00:52:57  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ …        ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 2        ┆ 2024-01-3 ┆ 2024-01-3 ┆ null      ┆ … ┆ 8.616667  ┆ 22.143133 ┆ 23        ┆ Wednesday │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 23:45:59  ┆ 23:54:36  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 1        ┆ 2024-01-3 ┆ 2024-01-3 ┆ null      ┆ … ┆ 14.75     ┆ 16.271186 ┆ 23        ┆ Wednesday │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 23:13:07  ┆ 23:27:52  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2        ┆ 2024-01-3 ┆ 2024-01-3 ┆ null      ┆ … ┆ 19.0      ┆ 10.515789 ┆ 23        ┆ Wednesday │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 23:19:00  ┆ 23:38:00  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2        ┆ 2024-01-3 ┆ 2024-01-3 ┆ null      ┆ … ┆ 17.85     ┆ 10.285714 ┆ 23        ┆ Wednesday │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 23:07:23  ┆ 23:25:14  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 1        ┆ 2024-01-3 ┆ 2024-02-0 ┆ null      ┆ … ┆ 15.083333 ┆ 32.220994 ┆ 23        ┆ Wednesday │\n",
       "│          ┆ 1         ┆ 1         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│          ┆ 23:58:25  ┆ 00:13:30  ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└──────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = df.with_columns([\n",
    "    # i) trip_duration_minutes: calculated from pickup and dropoff timestamps\n",
    "    (\n",
    "        (pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\"))\n",
    "        .dt.total_seconds() / 60).alias(\"trip_duration_minutes\"),\n",
    "\n",
    "    # j) trip_speed_mph: distance divided by duration (handle division by zero) \n",
    "    pl.when(\n",
    "        ((pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\"))\n",
    "         .dt.total_seconds()) > 0\n",
    "    )\n",
    "    .then(\n",
    "        pl.col(\"trip_distance\") /\n",
    "        (((pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\"))\n",
    "          .dt.total_seconds()) / 3600)\n",
    "    )\n",
    "    .otherwise(None)\n",
    "    .alias(\"trip_speed_mph\"),\n",
    "\n",
    "    # k) pickup_hour: hour of day (0-23) extracted from pickup timestamp\n",
    "    pl.col(\"tpep_pickup_datetime\").dt.hour().alias(\"pickup_hour\"),\n",
    "\n",
    "    # l) pickup_day_of_week: day name (Monday-Sunday) extracted from pickup timestamp\n",
    "    pl.col(\"tpep_pickup_datetime\").dt.strftime(\"%A\").alias(\"pickup_day_of_week\")\n",
    "])\n",
    "\n",
    "clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42e15f-bfac-4553-a3fe-6d18da41e64f",
   "metadata": {},
   "source": [
    "## SQL Analysis (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c01c2-12bf-4600-9456-4c76387bb31a",
   "metadata": {},
   "source": [
    "### m) What are the top 10 busiest pickup zones by total number of trips? (Include zone names from lookup table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911aa3ec-490c-4a11-b1aa-e47ef4390531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      zone_name  trips_count\n",
      "0                   JFK Airport       145240\n",
      "1                Midtown Center       143471\n",
      "2         Upper East Side South       142708\n",
      "3         Upper East Side North       136465\n",
      "4                  Midtown East       106717\n",
      "5     Times Sq/Theatre District       106324\n",
      "6  Penn Station/Madison Sq West       104523\n",
      "7           Lincoln Square East       104080\n",
      "8             LaGuardia Airport        89533\n",
      "9         Upper West Side South        88474\n"
     ]
    }
   ],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "result = con.execute('''\n",
    "    WITH trips AS (\n",
    "        SELECT * FROM 'clean'\n",
    "    ),\n",
    "    zones AS (\n",
    "        SELECT * FROM read_csv_auto('data/raw/taxi_zone_lookup.csv')\n",
    "    )\n",
    "    SELECT\n",
    "        z.Zone       AS zone_name,\n",
    "        COUNT(*)     AS trips_count\n",
    "    FROM trips t\n",
    "    JOIN zones z\n",
    "      ON t.PULocationID = z.LocationID    -- adjust column names here if needed\n",
    "    GROUP BY z.Zone\n",
    "    ORDER BY trips_count DESC\n",
    "    LIMIT 10;\n",
    "''').fetchdf()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530835e-9a9b-4c30-b13f-eddca8882e49",
   "metadata": {},
   "source": [
    "### n) What is the average fare amount for each hour of the day? (Order by hour) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96e4aad-e5a3-4f61-aa76-783fea18bb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pickup_hour  avg_fare_amount  trips_count\n",
      "0             0        19.202658        79094\n",
      "1             1        17.527296        53627\n",
      "2             2        16.482882        37517\n",
      "3             3        18.150135        24811\n",
      "4             4        22.518645        16742\n",
      "5             5        26.619918        18764\n",
      "6             6        21.650399        41429\n",
      "7             7        18.539180        83719\n",
      "8             8        17.654683       117209\n",
      "9             9        17.708415       128970\n",
      "10           10        17.753394       138778\n",
      "11           11        17.432059       150542\n",
      "12           12        17.476313       164559\n",
      "13           13        18.116046       169903\n",
      "14           14        18.945787       182898\n",
      "15           15        18.774634       189359\n",
      "16           16        19.121762       190201\n",
      "17           17        17.838371       206257\n",
      "18           18        16.723151       212788\n",
      "19           19        17.294794       184032\n",
      "20           20        17.695375       159989\n",
      "21           21        17.954276       160888\n",
      "22           22        18.688929       143261\n",
      "23           23        19.757696       109287\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "result = con.execute('''\n",
    "WITH trips AS (\n",
    "    SELECT * FROM read_parquet('data/raw/yellow_tripdata_2024-01.parquet')\n",
    ")\n",
    "SELECT\n",
    "    CAST(EXTRACT(hour FROM CAST(tpep_pickup_datetime AS TIMESTAMP)) AS INTEGER) AS pickup_hour,\n",
    "    AVG(CAST(fare_amount AS DOUBLE)) AS avg_fare_amount,\n",
    "    COUNT(*) AS trips_count\n",
    "FROM trips\n",
    "WHERE tpep_pickup_datetime IS NOT NULL\n",
    "  AND fare_amount IS NOT NULL\n",
    "GROUP BY pickup_hour\n",
    "ORDER BY pickup_hour;\n",
    "''').fetchdf()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d360b56-7906-4229-b250-c1f9577ba4cf",
   "metadata": {},
   "source": [
    "### o) What percentage of trips use each payment type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5323fa2f-1569-4cb4-b097-5cd7a9ba921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   payment_type  trips_count  pct_trips\n",
      "0             1      2319046      78.22\n",
      "1             2       439191      14.81\n",
      "2             0       140162       4.73\n",
      "3             4        46628       1.57\n",
      "4             3        19597       0.66\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "result = con.execute('''\n",
    "WITH trips AS (\n",
    "    SELECT * FROM read_parquet('data/raw/yellow_tripdata_2024-01.parquet')\n",
    "),\n",
    "counts AS (\n",
    "    SELECT\n",
    "        COALESCE(payment_type, 'UNKNOWN') AS payment_type,\n",
    "        COUNT(*) AS trips_count\n",
    "    FROM trips\n",
    "    GROUP BY COALESCE(payment_type, 'UNKNOWN')\n",
    ")\n",
    "SELECT\n",
    "    payment_type,\n",
    "    trips_count,\n",
    "    ROUND(100.0 * trips_count / SUM(trips_count) OVER (), 2) AS pct_trips\n",
    "FROM counts\n",
    "ORDER BY trips_count DESC;\n",
    "''').fetchdf()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa4199-a5f1-4e10-98d0-fd64bc034850",
   "metadata": {},
   "source": [
    "### p) What is the average tip percentage (tip_amount/fare_amount) by day of week, for credit card payments only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aec0f14-7233-4c10-84df-085edd7295d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   iso_dow   day_name  avg_tip_pct  avg_tip_ratio  trips_count\n",
      "0        1     Monday        39.83         0.3983       312675\n",
      "1        2    Tuesday        26.07         0.2607       362261\n",
      "2        3  Wednesday        25.76         0.2576       392563\n",
      "3        4   Thursday        30.11         0.3011       339953\n",
      "4        5     Friday        27.77         0.2777       321784\n",
      "5        6   Saturday        26.61         0.2661       327420\n",
      "6        7     Sunday        25.34         0.2534       262202\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "result = con.execute('''\n",
    "WITH trips AS (\n",
    "    SELECT * FROM read_parquet('data/raw/yellow_tripdata_2024-01.parquet')\n",
    ")\n",
    "SELECT\n",
    "    EXTRACT(ISODOW FROM CAST(tpep_pickup_datetime AS TIMESTAMP)) AS iso_dow,  -- 1=Mon .. 7=Sun\n",
    "    DAYNAME(CAST(tpep_pickup_datetime AS TIMESTAMP)) AS day_name,\n",
    "    ROUND(AVG(CAST(tip_amount AS DOUBLE) / NULLIF(CAST(fare_amount AS DOUBLE), 0)) * 100, 2) AS avg_tip_pct,\n",
    "    ROUND(AVG(CAST(tip_amount AS DOUBLE) / NULLIF(CAST(fare_amount AS DOUBLE), 0)), 4) AS avg_tip_ratio,\n",
    "    COUNT(*) AS trips_count\n",
    "FROM trips\n",
    "WHERE CAST(payment_type AS INTEGER) = 1    -- 1 = Credit card\n",
    "  AND tpep_pickup_datetime IS NOT NULL\n",
    "  AND fare_amount IS NOT NULL\n",
    "  AND CAST(fare_amount AS DOUBLE) > 0\n",
    "GROUP BY iso_dow, day_name\n",
    "ORDER BY iso_dow;\n",
    "''').fetchdf()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7d2b3-4f3c-4c02-9910-55261ae6cbb5",
   "metadata": {},
   "source": [
    "### q) What are the top 5 most common pickup-dropoff zone pairs? (Include zone names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "315495f6-f931-491e-9638-7fe31059a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pickup_zone           dropoff_zone  trips_count\n",
      "0  Upper East Side South  Upper East Side North        21883\n",
      "1  Upper East Side North  Upper East Side South        19402\n",
      "2  Upper East Side North  Upper East Side North        15955\n",
      "3  Upper East Side South  Upper East Side South        14938\n",
      "4         Midtown Center  Upper East Side South        10275\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "result = con.execute('''\n",
    "WITH trips AS (\n",
    "    SELECT * FROM read_parquet('data/raw/yellow_tripdata_2024-01.parquet')\n",
    "),\n",
    "zones AS (\n",
    "    SELECT * FROM read_csv_auto('data/raw/taxi_zone_lookup.csv')\n",
    ")\n",
    "SELECT\n",
    "  COALESCE(zp.Zone, CAST(t.PULocationID AS VARCHAR)) AS pickup_zone,\n",
    "  COALESCE(zd.Zone, CAST(t.DOLocationID AS VARCHAR)) AS dropoff_zone,\n",
    "  COUNT(*) AS trips_count\n",
    "FROM trips t\n",
    "LEFT JOIN zones zp ON t.PULocationID = zp.LocationID\n",
    "LEFT JOIN zones zd ON t.DOLocationID = zd.LocationID\n",
    "GROUP BY pickup_zone, dropoff_zone\n",
    "ORDER BY trips_count DESC\n",
    "LIMIT 5;\n",
    "''').fetchdf()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17cac3-8496-4c8b-ab1e-52f0e80e881f",
   "metadata": {},
   "source": [
    "# Part 3: Visualization Dashboard (40 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6afa12c-7b02-4793-abc6-c3883f660905",
   "metadata": {},
   "source": [
    "## Dashboard Structure (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345a192-830c-4ab8-8e48-f3d04f2e8f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
